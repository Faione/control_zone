\chapter{场景导向的调度定制}\label{chap:sched_policy}

\section{内核调度配置定制}

Linux调度子系统在设计时力图覆盖足够广泛的场景，但同时也提供了一些编译配置选项，用以针对场景进行优化，这些配置选项围绕时钟与抢占模型的设置展开。

时钟与Linux调度子系统密切相关，在第二章论述中提到，时钟中断是驱动Linux抢占式调度的核心机制，中断的频率决定了调度滴答的周期，而更高的时钟中断频率意味着系统的整体响应度越好。针对时钟中断，内核主要提供了两方面的配置，如表~\ref{tab:config_hz}所示，其中最直接的就是时钟中断的频率配置，内核提供了从100、250、300到1000四种不同的中断频率配置，以满足不同的场景下对于响应度的需求，而值得注意的是，由于内核在每个时钟中断处理中进行时间记账，并将两次时钟中断所间隔的时间片累计到当前进程的记账中，然而在实际情况下两次时钟中断中间很有可能夹杂了其他任务的处理，所以更高的时钟中断频率不仅能带来响应度的提升，还能够增强任务时间记账的准确性。其次，考虑到时钟中断会引入额外的计算开销，而在一些场景中这些开销是非必要的，如当CPU进入Idle状态，此时处理时钟中断不仅没有意义，还会增加系统的开销，因此内核提供了NO\_HZ相关配置来在特定场景下屏蔽时钟中断。

\begin{table}
    \bicaption{\quad 内核时钟中断配置}{\quad Kernel Clock Interrupt Configuration}% caption
    \label{tab:config_hz}
    \footnotesize% fontsize
    \setlength{\tabcolsep}{4pt}% column separation
    \renewcommand{\arraystretch}{1.5}% row space 
    \centering
    \begin{tabular}{lc}
        \hline
        配置名称 & 描述 \\
        \hline
        HZ\_100  & 配置时钟中断频率为100  \\
        HZ\_250  & 配置时钟中断频率为250 \\
        HZ\_300  & 配置时钟中断频率为300 \\
        HZ\_1000 & 配置时钟中断频率为1000 \\
        HZ\_PERIODIC & 永远不要忽略时钟中断 \\
        NO\_HZ\_IDLE & 忽略空闲CPU上的时钟中断 \\
        NO\_HZ\_FULL & 忽略空闲CPU，以及只有一个可运行任务CPU上的时钟中断 \\
        \hline
    \end{tabular}
\end{table}

抢占指执行任务时，允许高优先级的任务打断低优先级任务的执行，从而提供更好的响应度。用户态任务的执行总是能够被打断，而内核态任务的抢占则较为复杂，为此内核提供了抢占模型的编译配置，允许用户针对不同场景进行调整，相关配置如表~\ref{tab:config_preempt}所示， 内核提供了PREEMPT\_NONE、PREEMPT\_VOLUNTARY、PREEMPT以及PREEMPT\_RT四种抢占模式，四种模式下内核的响应度逐步增强。

\begin{table}
    \bicaption{\quad 内核抢占模式配置}{\quad Kernel Preemption Configuration}% caption
    \label{tab:config_preempt}
    \footnotesize% fontsize
    \setlength{\tabcolsep}{4pt}% column separation
    \renewcommand{\arraystretch}{1.5}% row space 
    \centering
    \begin{tabular}{lc}
        \hline
        配置名称 & 描述 \\
        \hline
        PREEMPT\_NONE  & 内核代码保持执行直到主动放弃CPU  \\
        PREEMPT\_VOLUNTARY  & 开启了内核代码中的抢占位点 \\
        PREEMPT  & 提供更多的内核代码抢占位点，实现完全抢占 \\
        PREEMPT\_RT & 进一步修改内核代码的实现，如锁机制，实现实时可抢占性 \\
        \hline
    \end{tabular}
\end{table}

时钟和抢占模式的配置能够决定系统的响应度，但实际的配置需要考虑使用的场景，如更高的HZ虽然能够提升系统的整体响应度，但由于时钟中断的增多，CPU在于有限的时间片内处理实际任务的时间就会变少，同时频繁的上下文切换也破坏了程序的时间局部性，从而影响到系统的整体吞吐。通常而言，较高的HZ与激进的抢占模式有利于降低延时，而较低的HZ与保守的抢占模式有利于系统的整体吞吐。对此Control Zone提供了两种预编译的内核，用于处理响应度优先与吞吐量优先两种场景。

\section{任务调度机制定制}

% Control Zone内调度
% - 互斥调度：用于SMT或保守调度策略
% - 有资源阈值的互斥调度: 资源达到阈值时，才进行互斥调度(系统资源达到阈值时，使能BPF Scheduler)
%   - Memory、CPU、Network、IO

\subsection{强隔离任务调度策略}

多任务调度场景可分为两种。第一种是单CPU调度队列上的任务调度，此时任务分时复用CPU资源，调度策略通常采用优先级机制，并优先为高优先级任务分配CPU时间。第二种则是多CPU调度队列上的任务调度，Linux内核在这种场景下通常会允许任务在一定程度上的并发， 同时利用复杂均衡机制来避免任务过度集中。

混部场景中LC应用通常需要优先被保护，Linux中允许用户声明nice值来表达这一需求，但内核当前调度机制在设计上存在一些缺陷，使得nice值并不能完全的对高优先级应用进行保护。首先，在单CPU调度队列上，以Fair调度类为例，即便为LC应用指定了最高优先级，LC应用仍存在被抢占的可能，这是由于Fair调度类在实现上就以公平为目标，因此会极力比避免出现饥饿的情况出现，体现在实现中，随LC应用的vruntime的不断积累，其优先性慢慢削弱直至被抢占。其次，在多CPU调度队列上，优先级通常不能跨CPU地作用，此时任务调度主要受负载均衡影响，在第一章的分析中，Linux提出了调度域这一概念，其中就考虑到了不同CPU之间潜在的资源竞争情况，如在一个SMT系统中，即便运行在不同Sibling上的任务存在优先级差异，但此时并不能保证高优先级任务的资源使用。

针对以上存在的问题，本研究设计了一种强隔离的任务调度策略，并实现为BPF Scheduler，来提供一种能够对高优先任务进行强保护的调度策略。首先，对于单CPU队列上的隔离性问题，由于Sched Ext调度类本身优先级较低，结合第二章中对调度循环的分析可知，在每次调度循环中， 较高优先级调度类中的任务总是会被优先选中，而同时只有在高优先级调度类中没有可执行的任务时，才会进入Sched Ext调度类的流程中，因此使用Sched Ext调度类天然就实现了单CPU调度队列上的隔离性。

\begin{algorithm}
    \caption{Pseudocode for Enhanced Task Scheduling Isolation Mechanism}
    \label{alg:enhance_sched_isolation}
    \begin{algorithmic}[1]
    \While{True}
        \If{$\text{CPU\_Acquired + CPU\_Idle} < \text{CPU\_Available}$ }
            \State Set Exclusive Flag;
            \For{each no\_hz core C}
                \State Send an IPI to envoke a rescheduling.
            \EndFor
            \State Yield to higher priority scheduler class;
        \EndIf
        \State Remove Exclusive Flag;
        \State Dispatched tasks;
    \EndWhile
    \end{algorithmic}
\end{algorithm}

多CPU调度队列上的强隔离实现则较为复杂。首先，在BPF Scheduler中，选择使用一个全局DSQ，并通过cpu\_acquire，cpu\_release两个回调函数来动态获取当前Sched Ext中所能使用的CPU数量，而具体的调度逻辑如伪代码~\ref{alg:enhance_sched_isolation}所示，首先BPF Scheduler会依据不同调度类CPU的数量，来判断是否有更高优先级调度类中的任务正在运行，一旦发现，则首先设置一个Exclusive Flag， 用于在其他回调中共享状态，其次，对于处于NoHZ状态下的CPU，发送IPI以注入一次重调度，而当系统中没有更高优先级调度类的任务运行时，则正常进行任务的分发。

\subsection{资源约束的任务调度策略}

% Control Zone级调度
% - 更具优先级的资源划分

高优先级任务在负载较低时可以低优先级任务并行，有利于提升整体资源的利用率。实现这一策略的核心在于让BPF调度器感知系统中高优先级任务的负载。首先，以LC应用为例，如Redis，其负载与网络请求强相关，而通过探测网络处理相关系统调用的使用情况，就能在一定程度上探测LC应用的负载，以epoll_wait系统调用为例，当系统中网络请求较低时，epoll_wait系统调用容易进入阻塞或超时控制状态，因此执行频率会大大降低，而当系统中网络请求较高时，epoll_wait系统调用几乎总能快速返回一个文件描述列表，而通过使用eBPF在此系统调用上插桩，就能够追踪系统调用的执行频率。基于这一机制可实现网络资源约束的任务调度策略，具体逻辑如伪代码~\ref{network_aware_sched}所示。

\begin{algorithm}
    \caption{Pseudocode for Enhanced Task Scheduling Isolation Mechanism}
    \label{alg:network_aware_sched}
    \begin{algorithmic}[1]
    \While{True}
        \State Read epoll wait delta from bpf map
        \If{$\text{EPOLL\_WAIT\_DELTA} < \text{EPOLL_WAIT_THRESHOLD}$ }
            \State Set Exclusive Flag;
            \For{each no\_hz core C}
                \State Send an IPI to envoke a rescheduling.
            \EndFor
            \State Yield to higher priority scheduler class;
        \EndIf
        \State Remove Exclusive Flag;
        \State Dispatched tasks;
    \EndWhile
    \end{algorithmic}
\end{algorithm}

在单CPU调度队列上，低于优先级任务总是会让出CPU资源，而具体差异体现在多CPU任务队列中，首先，通过eBPF插桩，记录固定时间内的epoll_wait系统调用次数，并保存到一个BPF Map中，当BPF Scheduler在接管CPU之后，会在每个调度循环中，首先读取BPF Map中的epoll_wait计数，并判断是否达到阈值，在阈值之下，允许调度任务，而当达在阈值之上，则放弃调度任务。其中eBPF采集以及阈值都可以根据高优先级的任务特性进行定制。

% \section{资源划分策略定制}

\section{本章小结}

本章主要首先针对画像分析中不同类型应用，基于内核中关于HZ与抢占模型的配置，设计了调度配置的定制优化，提供了吞吐量优先与响应度优先两种不同调度目标的内核配置。

随后分析混部场景下，高优先应用与低优先级应用基于BPF调度策略的QoS保障机制，首先提出较为粗粒度的强隔离BPF调度策略，能够感知高优先级任务的执行并主动出让全部CPU资源，从而保障高优先级应用的执行。随后为了提升整体的资源利用率，提出了基于资源约束的BPF调度策略，通过eBPF插桩感知系统中网络资源的使用，并通过与阈值的比较来允许一定程度的并发，从而在保障高优先级应用的同时，提升整体资源利用率。