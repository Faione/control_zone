\chapter{相关理论与技术}\label{chap:theories_tech}

\section{Linux调度子系统}

% 内核Core调度流程
% 影响内核调度的关键因素
% - HZ
% - 调度类/机制
% - 调度策略/算法
% - 高质量的操作系统组件，如调度器，则可能需要数十年的时间来完善\citep{agache2020firecracker}

调度子系统是Linux内核的一个重要组成部分，主要负责在多任务场景中为每个任务分配CPU资源。从架构上Linux调度子系统可分为三层，如图~\ref{fig:sched_arch}所示，最底层的Core调度机制提供了最基本的调度实现，包括抢占调度与非抢占调度，调度类层则针对不同调度目标，提供了包括实时调度、公平调度等特性，并且不同的调度类间存在优先级差异，最上层的调度策略则考虑到不同任务的差异，允许在调度类的基础之上，进一步划分任务的优先级，如在公平调度类中，就区分了NORMAL与BATCH类型的调度策略，同时也提供了系统调用来更灵活地设置任务静态优先级，Linux调度子系统各层为上层提供了基本机制，各层的协作为复杂的调度机制的实现提供了可能。

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{sched_arch}
    \bicaption{\quad 调度子系统架构}{\quad Scheduling subsystem architecture}
    \label{fig:sched_arch}
\end{figure}

Core层作为最基本的调度框架，是所有调度机制的基本入口。Core调度可分为非抢占式调度与非抢占式两个部分，其中非抢占式调度起源于批处理调度，在这一场景中，任务依次执行直到退出，如图~\ref{fig:core_batch_sched}所示，Core调度器需要为新fork出来的任务选择合适的CPU并进行入队，而当任务exit或主动出让CPU之后，则需要进行出入队管理，选择下一个继续的任务，并执行任务切换。而从上一个任务切换到下一个任务的过程即是调度循环，如图所示~\ref{fig:shcedule_loop}，调度循环通常由中断和系统调用驱动，并在处理完毕到返回用户任务之前的时刻，判断标志位来决定是否进入到调度循环。抢占式调度基于CPU的分时复用，一般由时钟中断驱动，如图~\ref{fig:schedule_tick}所示，在时钟中断处理函数中，会触发Core中的task\_tick，在其中就包含了当前任务所属调度类的响应回调，如更新当前任务的记账信息，以及决策是否进行抢占等。实际的抢占通过Core提供的resched\_curr实现，对于任务所在的CPU而言，该函数设置了抢占标记位，从而在时钟中断处理完毕后触发调度循环来完成任务的抢占。

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{core_batch_sched}
    \bicaption{\quad Core任务调度}{\quad Core scheduling tasks}
    \label{fig:core_batch_sched}
\end{figure}

调度类层在Core层之上实现。当前Linux中主要包含Stop、Deadline、Realtime、Fair、Idle五大调度类\citep{scheduler}。其中，Stop调度类的实现最简单，其本质上是从批处理调度中提出出来的逻辑。Fair调度类最复杂，使用到如红黑树等高级数据结构，并采用了较复杂的启发式算法与记账逻辑，力图以公平作为调度的目标，而其在多数场景下的稳定性能使得Fair长久以来都作为Linux中任务的默认调度类。其余调度类同样以不同的目标进行设计，如Deadline调度类能够保证任务在设定的周期中执行固定的时间。Linux为这些调度类设置的固定的优先级，如图所示~\ref{fig:sched_ext_priorty}, 每个调度循环都会从最高优先级的调度类开始尝试获取任务，这使得属于较高优先级调度类的任务总是能被优先执行。

\begin{figure}[!htbp]
    \centering 
    \includegraphics[width=0.7\textwidth]{shcedule_loop}
    \bicaption{\quad 调度循环}{\quad Schedule Loop}
    \label{fig:shcedule_loop}
\end{figure}

调度策略层基于调度类提高的框架实现，主要处理调度队列中的优先级机制，不同调度类中的实现各不相同。在Realtime调度类中，优先级体现为多个调度队列，选取任务时会从最高优先级的任务队列开始，这保证了高优先级的任务被优先执行，而在同一个任务队列中，调度策略则区分了任务轮换的形式，如FIFO采用先来先执行的逻辑，而RR则通过时间片轮转依次执行任务。在Fair调度类中，优先级机制的实现则完全不同，这种差异性来自于Fair调度类的算法实现上，如CFS调度算法会为每个任务维护一个vruntime，并优先选择vruntime最小的任务执行，在CFS算法中，优先级体现为vruntime积累的差异，高优先级的任务积累vruntime的速度会更慢，从而获取更多的执行机会，基于如上设计，Fair调度类提供了NORMAL与BATCH两种调度策略，对应为不同的优先级，来适配不同类型任务的调度场景。

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\textwidth]{schedule_tick}
    \bicaption{\quad 抢占调度时钟滴答}{\quad Schedule Tick}
    \label{fig:schedule_tick}
\end{figure}

\section{BPF技术}

% 定制内核的需求: 多样的硬件/多样的网络处理模式
% 内核模块: 灵活性高，但不安全，容易引发内核crash风险
% BPF在网络子系统引入，自定义网络包的处理流程
% 逐渐拓展到各个子系统中，但由于内核的审慎地设计，其他子系统中通常用来进行只读操作，如进行监测

eBPF（Extended Berkeley Packet Filter）是Linux内核中的一个轻量级、快速的64位的类RISC虚拟机\citep{sharaf2022extended}。当前eBPF当前已经成为在Linux内核运行时执行不可信的、由用户定义的专用代码的最佳实践与事实标准。其强大的性能、可移植性、灵活性与安全性得到了工业界和学术界的认可，并被广泛地用于更多的领域。

eBPF的前身是BPF（Berkeley Packet Filter），最早由McCanne和Jacobson设计并提出\citep{mccanne1993bsd}，起初BPF如其名称所示，用于在Linux网络子系统中对网络包进行灵活处理，而因其本身优异的设计，被Linux内核社区的贡献者扩展到内核的各个子系统中，来对内核功能进行定制化，为了与旧BPF技术进行区分，内核社区将早期的BPF技术成为cBPF（Classic Berkeley Packet Filter），而BPF和eBPF均指代最新的BPF技术，本文在后续说明中也采用这种做法。

BPF本身是一种指令集，最初设计时考虑到安全性与易用性，允许开发者使用C语言的子集进行编写，并能作为一种编译器后端指令集，由GCC等常用编译器编译为字节码\citep{ebpfguidence}。BPF采用字节码的处于两种原因，其一，方便内核验证器对代码进行验证，其二，与Java思想类似，利用字节码与语言虚拟机的组合提升可移植性。BPF虚拟机运行在内核中，Linux提供了相关的BPF系统调用来将字节码加载到内核中，并附加到代码中声明的钩子函数处，当对应函数触发时，内核态的BPF虚拟机就会执行附加的BPF字节码。整个过程中为了防止加载非法代码，内核首先会在加载BPF程序前对BPF字节码进行验证，判断是否符合在内核中执行的规范，如不允许死循环等，而在内核BPF虚拟机中执行时，会利用JIT技术将字节码映射到本机机器码，从而实现最佳的运行性能。

作为对内核功能的扩展，BPF技术经常会与内核模块进行比较。相较于内核模块，BPF技术具有更高的安全性，BPF程序安全性体现在三个方面。首先不同于内核模块，BPF程序在编写时所有的对内核数据、地址的访问都需要通过BPF Helper函数来实现，同时，部分BPF Helper函数设计时就考虑到并发性，极大减少了不安全代码的数量。其次，BPF程序在加载时会被内核验证，同时由于运行在虚拟机中，也增强了安全性。最后，BPF程序所能执行的位置由内核提供，这样也大大缩小了危险代码的影响范围。

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.6\textwidth]{bpf_user_kernel}
    \bicaption{\quad BPF中用户态与内核态的交互}{\quad Interaction between user mode and kernel mode in BPF}
    \label{fig:bpf_user_kernel}
\end{figure}

此外，BPF技术也具备高度的灵活性。这种灵活性体现在两个方面，首先对于用户态与内核态的交互，BPF技术运行由用户态的程序来加载BPF程序，由于BPF本身是字节码，因此能够借助libbpf等工具来对代码中的部分进行修改，这大大增强了BPF程序的灵活性。其次，如图~\ref{fig:bpf_user_kernel}所示，BPF技术允许通过BPF Map来实现用户态与内核的逻辑交互，一方面，内核态BPF程序所收集到的数据，能够借助BPF Map反馈给用户态程序进行处理，另一方面用户态程序也能通过BPF系统调用操作内BPF Map，对其中的内容进行读写，从而影响内核态BPF程序的行为。其次，BPF技术还允许BPF程序之间的交互，除上述通过BPF Map的交互外，如图~\ref{fig:bpf_to_bpf}所示，BPF程序还能够相互进行调用，从而进行数据传递，或通过多个BPF程序的调用，来突破内核对单个BPF函数的限制，从而实现更加复杂的代码逻辑。

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{bpf_to_bpf}
    \bicaption{\quad BPF程序之间的调用}{\quad Interaction between BPF programs}
    \label{fig:bpf_to_bpf}
\end{figure}

BPF字节码通常由内核中的BPF虚拟机解释执行，在一些内核路径上，这种低效的执行方式会严重影响系统性能，而为解决BPF字节码执行的性能问题，内核提供了JIT机制，当字节码首次被加载到内核时，会由内核中的BPF编译器编译为目标体系结构的机器代码，并替换原有的字节码保存在特定内存段中，当对应BPF Hook点触发时，同样先会进入BPF Wrapper函数，并捕获一些栈上数据，但在BPF代码执行处不再唤起虚拟机，而是直接跳转到BPF代码处执行，这使得BPF程序的执行效率大大提高。

然而BPF技术在设计上也存在先天的不足。首先，编写BPF程序所能使用的C语言的子集并非是图灵完备的，因此无法实现较为复杂的逻辑，同时BPF程序在栈空间上也有严格的要求，这些都与BPF子系统在安全与开销上设计目标一致，但是较弱的语言表达性往往使得BPF的使用场景有限。其次，BPF运行在内核环境中，依赖BPF Helper函数而不是系统调用来与内核沟通，而现行内核中Helper通常设计为针对有限的子系统受限制地开放，这也进一步限制了BPF程序的功能。然而BPF天然的灵活性与安全性，使得其得到的相当部分的内核开发者以及云原生社区的重视，内核社区中的BPF-next SIG就致力于解决BPF面临的种种问题，并尝试逐步地减少对BPF程序的限制，通过改造各个子系统来向BPF程序开放更多的功能。

开源社区中BPF的配套软件生态也被积极维护，当前主流的BPF软件生态可分为BCC\citep{bcc}与libbpf\citep{libbpf}，BCC提供了基于Pyhton语言的BPF元编程框架，开发者只需要定义少量的BPF程序逻辑，而完整的BPF代码生成、内核兼容性、编译环境选择均在Python解释执行的过程中进行，BCC预先准备了完毕的BPF编译环境，同时依赖目标机器上的内核功能来完成上述过程，这种做法解决的BPF程序与操作系统版本兼容性的问题，但也引入的大量的编译开销，并且元编程的形式也难以及时地适配BPF的新特性。Libbpf则采用了CO-RE的思路，即在本地就将BPF编译为字节码，并以只读数据的形式直接保存到用户代码中，用户代码可以通过Libbpf提供一系列辅助函数用来读写BPF字节码，并使用Libbpf提供的框架函数来完成BPF程序的加载、附加及删除。基于CO-RE形式的BPF程序由于不需要再次编译，因此在启动速度上更快，同时，目标机器上也不需要准备编译环境，减少了对软件环境的依赖。而为解决内核兼容性问题，Libbpf提供了bpftool来获取目标内核的数据结构信息，保证BPF程序在目标机器上正常允许，同时，开发者也可以在BPF程序中对内核兼容性进行声明，以提供更明确的兼容性信息。

\section{可扩展调度类}

Sched Ext可扩展调度类项目\citep{schedext}，最早于2022年由Meta工程师提出，用来解决Meta数据中心对调度的需求与Linux内核调度修改难、迭代慢的矛盾。Sched Ext项目的整体架构如图~\ref{fig:sched_ext_arch}所示，主要包含两个部分，其中内核部分为Ext调度类补丁集，由Meta工程师与内核社区工作者共同维护，当前尚未合入内核主线，用户态部分包含BPF Scheduler开发框架，基于Libbpf封装并主要支持C与Rust语言。Sched Ext项目当前已经在Meta集群中部署与测试，同时项目社区更新十分活跃。

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{sched_ext_arch}
    \bicaption{\quad Sched Ext架构}{\quad Sched Ext Arch}
    \label{fig:sched_ext_arch}
\end{figure}

Sched Ext项目启发自ghOSt\citep{humphries2021ghost}，通过设计一种插件化的调度类来提升Linux内核调度子系统开发的灵活性。在实现灵活的调度类上，Sched Ext与ghOSt存在两点主要差异，首先，Ext调度类在设计之初就尽可能地保持与Linux现有调度子系统的兼容性，如图~\ref{fig:sched_ext_priorty}所示，Ext调度类在优先级上有意地放置在Fair调度类与Idle调度类之间，当Ext调度类没有管理任务时，Linux调度子系统能够保持原有的所有特性，而当前Ext调度类管理了任务时，调度类会判断当前是否存在BPF Scheduler，并在BPF Scheduler没有加载时将任务交给Fair调度类处理，综合以上设计，Ext调度类做到对现有调度子系统的最小干扰。其次，Sched Ext项目使用BPF技术作为调度类扩展性的实现机制，相较于系统调用，一方面，BPF Scheduler能够保持在内核态运行，实现更小的调度开销，另一面，BPF本身就具有足够的表达能力，能够实现大部分的调度逻辑，同时BPF支持丰富的用户态交互手段，从而支持更灵活的用户态调度逻辑的表达。

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=1.0\textwidth]{sched_ext_priorty}
    \bicaption{\quad Sched Ext 调度优先级}{\quad Sched Ext Priorty}
    \label{fig:sched_ext_priorty}
\end{figure}

Ext调度类实现了sched\_class的完整方法并注册在内核调度子系统中。相较于其他调度类，Ext调度类的最大特点是调度的过程需要与BPF Scheduler协作完成。如图~\ref{fig:sched_ext_helpers}所示，为实现Ext调度类与BPF Schedule的交互，在数据通路上，Sched Ext项目提供了DSQ（Dispatch Queue），用于保存和传递任务，Ext调度类在初始化时，会为每个CPU创建一个Local DSQ，Ext调度类所管理的CPU需要从Local DSQ中获取任务并执行，BPF Scheduler也可以有自己的DSQ，用于在BPF程序中对任务进行管理，BPF Scheduler调度的底层逻辑就是BPF DSQ与Local DSQ之间任务的流转。在控制通路上，Sched Ext项目增加了一系列BPF Helper函数，主要包括DSQ的管理以及BPF Scheduler与CPU之间的交互，具体功能如下：

\begin{itemize}
    \item scx\_bpf\_create\_dsq： 定义一个DSQ（调度队列）来对任务进行管理。DSQ是BPF Scheduler管理任务的核心， BPF程序中可以保有不止一个DSQ，DSQ之间通过ID来进行区分，同时通过NUMA亲和性能够来保证访问DSQ内存的效率。
    \item scx\_bpf\_dispatch、scx\_bpf\_dispatch\_vtime：将任务调度到DSQ中。调度逻辑中可通过ID来指定目标DSQ以及任务的运行slice，其中slice可以设置为无限，此时任务不会被同队列中的其他任务抢占，而由BPF程序来决策抢占的时机。调度的任务通常以FIFO的形式入队，Sched Ext也提供了基于vtime的类CFS队列，以满足不同的BPF调度器的设计需求。

    \begin{figure}[!htbp]
        \centering
        \includegraphics[width=0.85\textwidth]{sched_ext_helpers}
        \bicaption{\quad Sched Ext BPF Helper 函数}{\quad Sched Ext BPF Helpers}
        \label{fig:sched_ext_helpers}
    \end{figure}

    
    \item scx\_bpf\_dispatch\_nr\_slots：查看CPU可调度的任务槽数。允许BPF程序检查当前CPU上可承载的任务余量，避免BPF Scheduler向目标CPU调度过多的任务。
    \item scx\_bpf\_dispatch\_cancel：撤销最近向CPU调度的一个任务。当任务的CPU亲和性发生变化时，BPF Scheduler原先调度的任务就应当进行变化，而此Helper函数便于BPF Scheduler撤销先前的调度决策。
    \item scx\_bpf\_consume：将BPF scheduler管理的任务调度给CPU。Sched Ext中的任务需要经过从BPF Scheduler的调度队列到CPU本地调度对队列的过程，这一过程的通过此Helper函数完成。
    \item scx\_bpf\_kick\_cpu：激活CPU的重调度。BPF程序能够通过此Helper函数唤醒一个idle CPU或者让繁忙的CPU进入调度循环中，为避免唤醒过程对调度延迟的影响，因此采用向中断队列添加延时任务的形式异步地设置唤醒任务，而在唤醒的过程主要通过Core框架提供的resched\_curr函数完成。
    \item scx\_bpf\_pick\_idle\_cpu：找到一个空闲的CPU。根据不同的调度目标，调度器应当避免任务过度的集中于同一个CPU，BPF Scheduler可以通过此Helper函数找到一个空闲CPU，将任务调度到此CPU上，并结合其他Helper函数来唤醒CPU。
\end{itemize}

Ext调度类与BPF Scheduler的关系如图~\ref{fig:ext_bpf}所示，其中DSQ与BPF Helper函数提供了最基本的交互逻辑，而为让BPF Scheduler参与到调度决策中，Ext调度类在调度的各个关键路径上都增加了BPF钩子函数位点，一方面，这些钩子函数能够向BPF Scheduler提供丰富的调度信息，另一方面，钩子函数的返回值也能够对Ext调度类的决策产生影响。Sched Ext项目中将这些钩子函数的集合定义为sched\_ext\_ops，每个BPF Scheduler可以根据需要，实现全部分或全的函数，每个函数的触发点与具体作用分别为：

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{ext_bpf}
    \bicaption{\quad Ext调度类与BPFScheduler协作}{\quad 
    Ext scheduling class cooperates with BPFScheduler}
    \label{fig:ext_bpf}
\end{figure}

\begin{itemize}
    \item select\_cpu：为唤醒的任务选择一个CPU。这一过程通常发送在SMP系统中，需要考虑任务的特性以及CPU的负载情况，选择的结果会影响到系统总体的性能，因此BPF所实现的逻辑不一定是最终的调度结果，实际的调度决策还会结合调度类本身的决策与BPF的决策。
    \item enqueue：处理任务的入队。调度类所管理的任务就绪时，会在入队的关键路径上触发BPF enqueue的执行，BPF中可将任务通过Helper函数发送到DSQ中，并进行后续的管理。
    \item dequeue：处理任务的出队。当需要对任务的调度属性，如优先级，进行修改时，便会触发任务出队逻辑，此时BPF中可以自定义优先级的更改逻辑，从而实现自定义的优先级功能。
    \item tick：处理任务滴答。Ext调度类在处理任务滴答时，触发此回调，并传入当前运行的任务。BPF Scheduler在tick中自定义任务滴答逻辑，如更新时间记账信息，同时，若在tick中将任务的时间片设置为0，则会在回调结束后直接触发一次dispatch，让CPU选择Ext调度队列中的另一个任务执行。
    \item dispatch：处理任务的调度。BPF可以通过创建DSQ来持有任务，同时每个CPU也拥有自己的Local DSQ来保存要执行的任务，当CPU的Local DSQ为空时，就会触发BPF的任务调度，期望从BPF的DSQ中获取将要执行的任务。BPF可以一次性调度多个任务，从而增加调度的效率，而任务的选择则完全由BPF程序决定。
    \item runnable、running、stopping、quiescent：追踪任务的状态。任务通常会在如下三种情况进入Runnable状态，首先是被唤醒时，如从等待IO的状态中恢复，其次是从其他CPU迁入时，以及从队列中暂移后恢复时，如修改任务优先级时，这些情况通过flag进行区分，并触发runnable的执行，quiescent的触发与上述时机完全相反，其他函数则在更细致的时机触发以反映任务的状态，其中running对应任务将要在CPU上运行时，stopping对应任务将要停止运行时。BPF通过这些函数来追踪任务的状态变化，从而辅助进行自定义的调度过程。
    \item yield：处理任务出让CPU资源。Linux提供了sched\_yield系统调用，允许任务主动放弃CPU资源，通常用于多线程中。sched\_yield并不保证线程组中的其他线程立即执行，而是将决策交给调度器，Sched Ext中此决策过程可完全由BPF程序实现。
    \item cpu\_acquire，cpu\_release：追踪调度类的CPU进出情况。当一个CPU对BPF Scheduler变为可用时，会触发cpu\_acquire，反之则触发cpu\_release。如当正在运行的EXT调度类的任务被更高优先级调度类的其他任务抢占时，当前CPU对于BPF Scheduler来说就变为不可用了。
    \item init\_task、exit\_task：追踪任务的创建与退出。调度类中每当有任务新创建或退出时触发，BPF可追踪这些事件，从而能够在BPF Scheduler中为每个任务定义额外的信息用于后续的调度过程。
\end{itemize}

BPF Schedueler为开发者提供了多样的模式来对Ext调度类进行定制。首先，最直接的方式是仅设置任务的调度类为Ext，由于并没有加载BPF Scheduler，Ext调度类会将任务委托给Fair调度类处理，这种模式主要用于对于任务调度状态的追踪。其次，开发者可以编写BPF Scheduler并加载到内核中，此时Ext调度类开始真正地调度任务，并可以根据开发者的需要来接管Fair调度类以及Idle调度类中的任务，从而方便地对系统中的主要任务进行管理。最后，开发者可以利用BPF程序的交互接口，进一步在用户态实现更复杂的调度逻辑，Sched Ext项目提供了一套基于Rust语言与Libbpf的用户态调度器开发框架，其实现的核心在于通过BPF Map和Ring Buffer在用户态代码与BPF Scheduler之间共享任务与调度事件，这种方式可以避免内核态以及BPF程序在表达能力上的限制，从而实现俄共灵活的调度策略。

\section{沙箱技术}

% 系统级沙箱(虚拟机)，容器级沙箱(进程)
% 安全性: 虚拟机 > 沙箱
% - 容器类沙箱缺乏调度机制的支持，而对调度机制的修改需要更安全的沙箱环境来限制风险的传播

沙箱技术是一种安全机制，用于隔离运行环境，以便在受限的环境中执行不受信任的程序或代码，并对软件所能使用的资源，如CPU、内存、网络等进行限制，从而防止这些程序或代码对主机上其他任务造成影响或危害。沙箱技术提供了安全运行应用提供了基础，数据中心中常见的沙箱技术包括虚拟机技术与容器技术。

虚拟机技术是数据中心应用广泛的沙箱技术，虚拟机指通过软硬件的手段，在已运行的系统中模拟出一个硬件环境，并能够支持运行其他系统。虚拟机技术最早提出于\citep{bugnion1997disco}, 用于解决操作系统迭代速度与硬件更新速度不匹配的问题，并希望依托虚拟设备与虚拟机环境，来提升操作系统的迭代速度。而随着虚拟化技术的不断发展，虚拟化开销也逐渐降低，其中软件上，从运行在裸金属上的Type 1虚拟化技术Xen\citep{barham2003xen}，到能够将Linux转化为一个Hyeprvisor的Type 2虚拟化技术KVM\citep{kivity2007kvm}，虚拟化软件技术的不断发展使得虚拟机的管理越来越简单，而在硬件上，基于SRIOV的设备直通手段\citep{dong2012high}，让虚拟机能够直接接触物理设备，从而达到几乎与裸金属操作系统相近的性能。同时虚拟机具备较高的隔离性，能够较好的处理多租户场景下的安全性问题，以上这些为虚拟机在云厂商中较长时间的广泛使用提供了基础。

容器技术期望提供一种进程级别的沙箱，来隔离运行在同一系统上的不同进程。程序的执行依赖一定的运行环境，Linux早期的发展过程中，由于包管理工具的不成熟，使得不同程序之间会相互污染运行环境，造成程序的运行错误或失败，为解决此问题，Linux提供了chroot工具以及相关的系统调用，允许为程序设置不同的根目录，从而让不同的程序运行在满足各自运行环境的根目录中，解决相互之间的依赖污染问题。chroot实质是一种文件系统的隔离机制，而后在Linux的不断发展过程中，越来越多的隔离机制被实现，其中Cgroup与Namespace机制构成了现代容器技术的基础。Namespace技术提供了Mount、UTS、IPC、PID、Network及User等系统资源的隔离，使得容器中的进程几乎认为自己运行在一个独立的系统环境中，而Cgroup技术则提供了CPU、Memory、BlkIO、NetIO、Devices等硬件资源的隔离，使得容器中的进程能够运行在一个受限制的硬件资源环境中。Overlayfs技术能够将多个目录文件叠加为一个统一的目录文件，并允许各层之间有不同的读写行为，同时借助COW（写时复制）来解决各层文件共享的问题，而这一技术为容器镜像技术提供了基础，而借助标准化的OCI镜像，容器得以方便地在不同的发行版、软件环境的Linux系统中传播。容器的易用性催生了容器编排技术，而以Kubernetes为主的容器编排技术在数据中心中占有越来越多的比例。

容器在本质上仍然是普通进程，因此在执行效率上高于虚拟机，但由于进程共享了相同的内核，因而不可避免地保有进程的安全问题，而这点在多租户场景的云环境中尤为凸显。现代容器技术提供了各种各样的手段提升容器的安全性，如限制容器中进程能够使用的系统调用等，然而这种方式牺牲了容器的通用性，容易导致部分容器的运行异常。安全容器的提出就试图解决这一问题，安全容器本质上是一个高度裁剪的虚拟机，运行着一个同样高度裁剪的内核，这样的处理大大精简了虚拟化环境，从而能够在提升虚拟机启动速度的同时，减少虚拟机本身的内存占用，同时，绝大多数的容器镜像都包含了一个较为完整的根文件系统，稍作处理后就可以作为一个虚拟机镜像来提供虚拟机使用。安全容器作为一种沙箱技术，结合了虚拟机的强隔离性与容器的易用性，在虚拟机运行时上，阿里的RunD\citep{li2022rund}、AWS的Firecraker\citep{agache2020firecracker}等轻量级虚拟机运行时大大降低了虚拟化的额外开销，而基于轻量级虚拟机技术所构建的安全容器如gVisor、KataContainer\citep{randazzo2019kata}等也逐渐在数据中心中应用。

\section{本章小结}

本章节首先介绍了Linux中的调度子系统和eBPF技术，说明了当前Linux调度子系统的基本原理以及发展状况，并阐述了eBPF技术作为一种集性能、灵活性于安全性于一体的内核扩展机制的工作原理。

随后介绍了将eBPF技术引入到调度子系统的可扩展调度类项目Sched Extend，分析了其核心的Ext调度类与BPF Scheduler的实现原理，阐述了其所带来的灵活的内核调度机制设计，有利于加速当前Linux调度子系统的迭代，并为在各种场景下的定制化调度优化提供了可能。

最后介绍了常见的沙箱技术，以容器技术、虚拟机技术、安全容器技术为主，分析了不同沙箱技术所重点解决的问题，以及存在的优劣势。以上这些相关技术构成了后续Control Zone设计实现的基础。